digraph {
	graph [size="225.15,225.15"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2617095600304 [label="
 ()" fillcolor=darkolivegreen1]
	2617095586576 [label=MeanBackward0]
	2617095586672 -> 2617095586576
	2617095586672 [label=NativeBatchNormBackward0]
	2617095586480 -> 2617095586672
	2617095586480 [label=AddmmBackward0]
	2617095586768 -> 2617095586480
	2617014080832 [label="fc.bias
 (512)" fillcolor=lightblue]
	2617014080832 -> 2617095586768
	2617095586768 [label=AccumulateGrad]
	2617095586720 -> 2617095586480
	2617095586720 [label=ToCopyBackward0]
	2617095586864 -> 2617095586720
	2617095586864 [label=ReshapeAliasBackward0]
	2617095587056 -> 2617095586864
	2617095587056 [label=CudnnBatchNormBackward0]
	2617095587152 -> 2617095587056
	2617095587152 [label=AddBackward0]
	2617095587344 -> 2617095587152
	2617095587344 [label=CudnnBatchNormBackward0]
	2617095587488 -> 2617095587344
	2617095587488 [label=ConvolutionBackward0]
	2617095587680 -> 2617095587488
	2617095587680 [label=PreluBackward0]
	2617095587824 -> 2617095587680
	2617095587824 [label=CudnnBatchNormBackward0]
	2617095587968 -> 2617095587824
	2617095587968 [label=ConvolutionBackward0]
	2617095588160 -> 2617095587968
	2617095588160 [label=CudnnBatchNormBackward0]
	2617095587296 -> 2617095588160
	2617095587296 [label=AddBackward0]
	2617095588448 -> 2617095587296
	2617095588448 [label=CudnnBatchNormBackward0]
	2617095588592 -> 2617095588448
	2617095588592 [label=ConvolutionBackward0]
	2617095588784 -> 2617095588592
	2617095588784 [label=PreluBackward0]
	2617095588928 -> 2617095588784
	2617095588928 [label=CudnnBatchNormBackward0]
	2617095589072 -> 2617095588928
	2617095589072 [label=ConvolutionBackward0]
	2617095589264 -> 2617095589072
	2617095589264 [label=CudnnBatchNormBackward0]
	2617095588400 -> 2617095589264
	2617095588400 [label=AddBackward0]
	2617095589552 -> 2617095588400
	2617095589552 [label=CudnnBatchNormBackward0]
	2617095589696 -> 2617095589552
	2617095589696 [label=ConvolutionBackward0]
	2617095589840 -> 2617095589696
	2617095589840 [label=PreluBackward0]
	2617095655632 -> 2617095589840
	2617095655632 [label=CudnnBatchNormBackward0]
	2617095655776 -> 2617095655632
	2617095655776 [label=ConvolutionBackward0]
	2617095655968 -> 2617095655776
	2617095655968 [label=CudnnBatchNormBackward0]
	2617095656112 -> 2617095655968
	2617095656112 [label=AddBackward0]
	2617095656304 -> 2617095656112
	2617095656304 [label=CudnnBatchNormBackward0]
	2617095656448 -> 2617095656304
	2617095656448 [label=ConvolutionBackward0]
	2617095656640 -> 2617095656448
	2617095656640 [label=PreluBackward0]
	2617095656784 -> 2617095656640
	2617095656784 [label=CudnnBatchNormBackward0]
	2617095656928 -> 2617095656784
	2617095656928 [label=ConvolutionBackward0]
	2617095657120 -> 2617095656928
	2617095657120 [label=CudnnBatchNormBackward0]
	2617095656256 -> 2617095657120
	2617095656256 [label=AddBackward0]
	2617095657408 -> 2617095656256
	2617095657408 [label=CudnnBatchNormBackward0]
	2617095657552 -> 2617095657408
	2617095657552 [label=ConvolutionBackward0]
	2617095657744 -> 2617095657552
	2617095657744 [label=PreluBackward0]
	2617095657888 -> 2617095657744
	2617095657888 [label=CudnnBatchNormBackward0]
	2617095658032 -> 2617095657888
	2617095658032 [label=ConvolutionBackward0]
	2617095658224 -> 2617095658032
	2617095658224 [label=CudnnBatchNormBackward0]
	2617095657360 -> 2617095658224
	2617095657360 [label=AddBackward0]
	2617095658512 -> 2617095657360
	2617095658512 [label=CudnnBatchNormBackward0]
	2617095658656 -> 2617095658512
	2617095658656 [label=ConvolutionBackward0]
	2617095658848 -> 2617095658656
	2617095658848 [label=PreluBackward0]
	2617095658992 -> 2617095658848
	2617095658992 [label=CudnnBatchNormBackward0]
	2617095659136 -> 2617095658992
	2617095659136 [label=ConvolutionBackward0]
	2617095659328 -> 2617095659136
	2617095659328 [label=CudnnBatchNormBackward0]
	2617095658464 -> 2617095659328
	2617095658464 [label=AddBackward0]
	2617095671968 -> 2617095658464
	2617095671968 [label=CudnnBatchNormBackward0]
	2617095672112 -> 2617095671968
	2617095672112 [label=ConvolutionBackward0]
	2617095672304 -> 2617095672112
	2617095672304 [label=PreluBackward0]
	2617095672448 -> 2617095672304
	2617095672448 [label=CudnnBatchNormBackward0]
	2617095672592 -> 2617095672448
	2617095672592 [label=ConvolutionBackward0]
	2617095672784 -> 2617095672592
	2617095672784 [label=CudnnBatchNormBackward0]
	2617095671920 -> 2617095672784
	2617095671920 [label=AddBackward0]
	2617095673072 -> 2617095671920
	2617095673072 [label=CudnnBatchNormBackward0]
	2617095673216 -> 2617095673072
	2617095673216 [label=ConvolutionBackward0]
	2617095673408 -> 2617095673216
	2617095673408 [label=PreluBackward0]
	2617095673552 -> 2617095673408
	2617095673552 [label=CudnnBatchNormBackward0]
	2617095673696 -> 2617095673552
	2617095673696 [label=ConvolutionBackward0]
	2617095673888 -> 2617095673696
	2617095673888 [label=CudnnBatchNormBackward0]
	2617095673024 -> 2617095673888
	2617095673024 [label=AddBackward0]
	2617095674176 -> 2617095673024
	2617095674176 [label=CudnnBatchNormBackward0]
	2617095674320 -> 2617095674176
	2617095674320 [label=ConvolutionBackward0]
	2617095674512 -> 2617095674320
	2617095674512 [label=PreluBackward0]
	2617095674656 -> 2617095674512
	2617095674656 [label=CudnnBatchNormBackward0]
	2617095674800 -> 2617095674656
	2617095674800 [label=ConvolutionBackward0]
	2617095674992 -> 2617095674800
	2617095674992 [label=CudnnBatchNormBackward0]
	2617095674128 -> 2617095674992
	2617095674128 [label=AddBackward0]
	2617095675280 -> 2617095674128
	2617095675280 [label=CudnnBatchNormBackward0]
	2617095675424 -> 2617095675280
	2617095675424 [label=ConvolutionBackward0]
	2617095675616 -> 2617095675424
	2617095675616 [label=PreluBackward0]
	2617095675760 -> 2617095675616
	2617095675760 [label=CudnnBatchNormBackward0]
	2617095675856 -> 2617095675760
	2617095675856 [label=ConvolutionBackward0]
	2617095684352 -> 2617095675856
	2617095684352 [label=CudnnBatchNormBackward0]
	2617095675232 -> 2617095684352
	2617095675232 [label=AddBackward0]
	2617095684640 -> 2617095675232
	2617095684640 [label=CudnnBatchNormBackward0]
	2617095684784 -> 2617095684640
	2617095684784 [label=ConvolutionBackward0]
	2617095684976 -> 2617095684784
	2617095684976 [label=PreluBackward0]
	2617095685120 -> 2617095684976
	2617095685120 [label=CudnnBatchNormBackward0]
	2617095685264 -> 2617095685120
	2617095685264 [label=ConvolutionBackward0]
	2617095685456 -> 2617095685264
	2617095685456 [label=CudnnBatchNormBackward0]
	2617095684592 -> 2617095685456
	2617095684592 [label=AddBackward0]
	2617095685744 -> 2617095684592
	2617095685744 [label=CudnnBatchNormBackward0]
	2617095685888 -> 2617095685744
	2617095685888 [label=ConvolutionBackward0]
	2617095686080 -> 2617095685888
	2617095686080 [label=PreluBackward0]
	2617095686224 -> 2617095686080
	2617095686224 [label=CudnnBatchNormBackward0]
	2617095686368 -> 2617095686224
	2617095686368 [label=ConvolutionBackward0]
	2617095686560 -> 2617095686368
	2617095686560 [label=CudnnBatchNormBackward0]
	2617095685696 -> 2617095686560
	2617095685696 [label=AddBackward0]
	2617095686848 -> 2617095685696
	2617095686848 [label=CudnnBatchNormBackward0]
	2617095686992 -> 2617095686848
	2617095686992 [label=ConvolutionBackward0]
	2617095687184 -> 2617095686992
	2617095687184 [label=PreluBackward0]
	2617095687328 -> 2617095687184
	2617095687328 [label=CudnnBatchNormBackward0]
	2617095687472 -> 2617095687328
	2617095687472 [label=ConvolutionBackward0]
	2617095687664 -> 2617095687472
	2617095687664 [label=CudnnBatchNormBackward0]
	2617095686800 -> 2617095687664
	2617095686800 [label=AddBackward0]
	2617095687952 -> 2617095686800
	2617095687952 [label=CudnnBatchNormBackward0]
	2617095688096 -> 2617095687952
	2617095688096 [label=ConvolutionBackward0]
	2617095700640 -> 2617095688096
	2617095700640 [label=PreluBackward0]
	2617095700784 -> 2617095700640
	2617095700784 [label=CudnnBatchNormBackward0]
	2617095700928 -> 2617095700784
	2617095700928 [label=ConvolutionBackward0]
	2617095701120 -> 2617095700928
	2617095701120 [label=CudnnBatchNormBackward0]
	2617095687904 -> 2617095701120
	2617095687904 [label=AddBackward0]
	2617095701408 -> 2617095687904
	2617095701408 [label=CudnnBatchNormBackward0]
	2617095701552 -> 2617095701408
	2617095701552 [label=ConvolutionBackward0]
	2617095701744 -> 2617095701552
	2617095701744 [label=PreluBackward0]
	2617095701888 -> 2617095701744
	2617095701888 [label=CudnnBatchNormBackward0]
	2617095702032 -> 2617095701888
	2617095702032 [label=ConvolutionBackward0]
	2617095702224 -> 2617095702032
	2617095702224 [label=CudnnBatchNormBackward0]
	2617095701360 -> 2617095702224
	2617095701360 [label=AddBackward0]
	2617095702512 -> 2617095701360
	2617095702512 [label=CudnnBatchNormBackward0]
	2617095702656 -> 2617095702512
	2617095702656 [label=ConvolutionBackward0]
	2617095702848 -> 2617095702656
	2617095702848 [label=PreluBackward0]
	2617095702992 -> 2617095702848
	2617095702992 [label=CudnnBatchNormBackward0]
	2617095703136 -> 2617095702992
	2617095703136 [label=ConvolutionBackward0]
	2617095703328 -> 2617095703136
	2617095703328 [label=CudnnBatchNormBackward0]
	2617095702464 -> 2617095703328
	2617095702464 [label=AddBackward0]
	2617095703616 -> 2617095702464
	2617095703616 [label=CudnnBatchNormBackward0]
	2617095703760 -> 2617095703616
	2617095703760 [label=ConvolutionBackward0]
	2617095703952 -> 2617095703760
	2617095703952 [label=PreluBackward0]
	2617095704096 -> 2617095703952
	2617095704096 [label=CudnnBatchNormBackward0]
	2617095704240 -> 2617095704096
	2617095704240 [label=ConvolutionBackward0]
	2617095704432 -> 2617095704240
	2617095704432 [label=CudnnBatchNormBackward0]
	2617095704528 -> 2617095704432
	2617095704528 [label=AddBackward0]
	2617095717120 -> 2617095704528
	2617095717120 [label=CudnnBatchNormBackward0]
	2617095717264 -> 2617095717120
	2617095717264 [label=ConvolutionBackward0]
	2617095717456 -> 2617095717264
	2617095717456 [label=PreluBackward0]
	2617095717600 -> 2617095717456
	2617095717600 [label=CudnnBatchNormBackward0]
	2617095717744 -> 2617095717600
	2617095717744 [label=ConvolutionBackward0]
	2617095717936 -> 2617095717744
	2617095717936 [label=CudnnBatchNormBackward0]
	2617095717072 -> 2617095717936
	2617095717072 [label=AddBackward0]
	2617095718224 -> 2617095717072
	2617095718224 [label=CudnnBatchNormBackward0]
	2617095718368 -> 2617095718224
	2617095718368 [label=ConvolutionBackward0]
	2617095718560 -> 2617095718368
	2617095718560 [label=PreluBackward0]
	2617095718704 -> 2617095718560
	2617095718704 [label=CudnnBatchNormBackward0]
	2617095718848 -> 2617095718704
	2617095718848 [label=ConvolutionBackward0]
	2617095719040 -> 2617095718848
	2617095719040 [label=CudnnBatchNormBackward0]
	2617095718176 -> 2617095719040
	2617095718176 [label=AddBackward0]
	2617095719328 -> 2617095718176
	2617095719328 [label=CudnnBatchNormBackward0]
	2617095719472 -> 2617095719328
	2617095719472 [label=ConvolutionBackward0]
	2617095719664 -> 2617095719472
	2617095719664 [label=PreluBackward0]
	2617095719808 -> 2617095719664
	2617095719808 [label=CudnnBatchNormBackward0]
	2617095719952 -> 2617095719808
	2617095719952 [label=ConvolutionBackward0]
	2617095720144 -> 2617095719952
	2617095720144 [label=CudnnBatchNormBackward0]
	2617095719280 -> 2617095720144
	2617095719280 [label=AddBackward0]
	2617095720432 -> 2617095719280
	2617095720432 [label=CudnnBatchNormBackward0]
	2617095720576 -> 2617095720432
	2617095720576 [label=ConvolutionBackward0]
	2617095720768 -> 2617095720576
	2617095720768 [label=PreluBackward0]
	2617095720912 -> 2617095720768
	2617095720912 [label=CudnnBatchNormBackward0]
	2617095733408 -> 2617095720912
	2617095733408 [label=ConvolutionBackward0]
	2617095733600 -> 2617095733408
	2617095733600 [label=CudnnBatchNormBackward0]
	2617095733744 -> 2617095733600
	2617095733744 [label=AddBackward0]
	2617095733936 -> 2617095733744
	2617095733936 [label=CudnnBatchNormBackward0]
	2617095734080 -> 2617095733936
	2617095734080 [label=ConvolutionBackward0]
	2617095734272 -> 2617095734080
	2617095734272 [label=PreluBackward0]
	2617095734416 -> 2617095734272
	2617095734416 [label=CudnnBatchNormBackward0]
	2617095734560 -> 2617095734416
	2617095734560 [label=ConvolutionBackward0]
	2617095734752 -> 2617095734560
	2617095734752 [label=CudnnBatchNormBackward0]
	2617095733888 -> 2617095734752
	2617095733888 [label=AddBackward0]
	2617095735040 -> 2617095733888
	2617095735040 [label=CudnnBatchNormBackward0]
	2617095735184 -> 2617095735040
	2617095735184 [label=ConvolutionBackward0]
	2617095735376 -> 2617095735184
	2617095735376 [label=PreluBackward0]
	2617095735520 -> 2617095735376
	2617095735520 [label=CudnnBatchNormBackward0]
	2617095735664 -> 2617095735520
	2617095735664 [label=ConvolutionBackward0]
	2617095735856 -> 2617095735664
	2617095735856 [label=CudnnBatchNormBackward0]
	2617095734992 -> 2617095735856
	2617095734992 [label=AddBackward0]
	2617095736144 -> 2617095734992
	2617095736144 [label=CudnnBatchNormBackward0]
	2617095736288 -> 2617095736144
	2617095736288 [label=ConvolutionBackward0]
	2617095736480 -> 2617095736288
	2617095736480 [label=PreluBackward0]
	2617095736624 -> 2617095736480
	2617095736624 [label=CudnnBatchNormBackward0]
	2617095736768 -> 2617095736624
	2617095736768 [label=ConvolutionBackward0]
	2617095736960 -> 2617095736768
	2617095736960 [label=CudnnBatchNormBackward0]
	2617095737104 -> 2617095736960
	2617095737104 [label=PreluBackward0]
	2617095737296 -> 2617095737104
	2617095737296 [label=CudnnBatchNormBackward0]
	2617095745696 -> 2617095737296
	2617095745696 [label=ConvolutionBackward0]
	2617095745888 -> 2617095745696
	2617095745888 [label=ToCopyBackward0]
	2617095745984 -> 2617095745888
	2616975740496 [label="conv1.weight
 (64, 3, 3, 3)" fillcolor=lightblue]
	2616975740496 -> 2617095745984
	2617095745984 [label=AccumulateGrad]
	2617095745648 -> 2617095737296
	2616975740336 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2616975740336 -> 2617095745648
	2617095745648 [label=AccumulateGrad]
	2617095745600 -> 2617095737296
	2616975740576 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2616975740576 -> 2617095745600
	2617095745600 [label=AccumulateGrad]
	2617095737248 -> 2617095737104
	2617095737248 [label=ToCopyBackward0]
	2617095745936 -> 2617095737248
	2616975761632 [label="prelu.weight
 (64)" fillcolor=lightblue]
	2616975761632 -> 2617095745936
	2617095745936 [label=AccumulateGrad]
	2617095737056 -> 2617095736960
	2616975762192 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2616975762192 -> 2617095737056
	2617095737056 [label=AccumulateGrad]
	2617095737008 -> 2617095736960
	2616975762272 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2616975762272 -> 2617095737008
	2617095737008 [label=AccumulateGrad]
	2617095736912 -> 2617095736768
	2617095736912 [label=ToCopyBackward0]
	2617095737200 -> 2617095736912
	2616975762752 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2616975762752 -> 2617095737200
	2617095737200 [label=AccumulateGrad]
	2617095736720 -> 2617095736624
	2616975762672 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2616975762672 -> 2617095736720
	2617095736720 [label=AccumulateGrad]
	2617095736672 -> 2617095736624
	2616975762832 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2616975762832 -> 2617095736672
	2617095736672 [label=AccumulateGrad]
	2617095736576 -> 2617095736480
	2617095736576 [label=ToCopyBackward0]
	2617095737152 -> 2617095736576
	2616975763152 [label="layer1.0.prelu.weight
 (64)" fillcolor=lightblue]
	2616975763152 -> 2617095737152
	2617095737152 [label=AccumulateGrad]
	2617095736432 -> 2617095736288
	2617095736432 [label=ToCopyBackward0]
	2617095736816 -> 2617095736432
	2616975763392 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2616975763392 -> 2617095736816
	2617095736816 [label=AccumulateGrad]
	2617095736240 -> 2617095736144
	2616975763312 [label="layer1.0.bn3.weight
 (64)" fillcolor=lightblue]
	2616975763312 -> 2617095736240
	2617095736240 [label=AccumulateGrad]
	2617095736192 -> 2617095736144
	2616975763472 [label="layer1.0.bn3.bias
 (64)" fillcolor=lightblue]
	2616975763472 -> 2617095736192
	2617095736192 [label=AccumulateGrad]
	2617095736096 -> 2617095734992
	2617095736096 [label=CudnnBatchNormBackward0]
	2617095736864 -> 2617095736096
	2617095736864 [label=ConvolutionBackward0]
	2617095737104 -> 2617095736864
	2617095745840 -> 2617095736864
	2617095745840 [label=ToCopyBackward0]
	2617095745792 -> 2617095745840
	2616975761712 [label="layer1.0.downsample.0.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2616975761712 -> 2617095745792
	2617095745792 [label=AccumulateGrad]
	2617095736384 -> 2617095736096
	2616975761792 [label="layer1.0.downsample.1.weight
 (64)" fillcolor=lightblue]
	2616975761792 -> 2617095736384
	2617095736384 [label=AccumulateGrad]
	2617095736336 -> 2617095736096
	2616975761872 [label="layer1.0.downsample.1.bias
 (64)" fillcolor=lightblue]
	2616975761872 -> 2617095736336
	2617095736336 [label=AccumulateGrad]
	2617095736000 -> 2617095735856
	2616975763792 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2616975763792 -> 2617095736000
	2617095736000 [label=AccumulateGrad]
	2617095735952 -> 2617095735856
	2616975763872 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2616975763872 -> 2617095735952
	2617095735952 [label=AccumulateGrad]
	2617095735808 -> 2617095735664
	2617095735808 [label=ToCopyBackward0]
	2617095736528 -> 2617095735808
	2616975764352 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2616975764352 -> 2617095736528
	2617095736528 [label=AccumulateGrad]
	2617095735616 -> 2617095735520
	2616975764272 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2616975764272 -> 2617095735616
	2617095735616 [label=AccumulateGrad]
	2617095735568 -> 2617095735520
	2616975764432 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2616975764432 -> 2617095735568
	2617095735568 [label=AccumulateGrad]
	2617095735472 -> 2617095735376
	2617095735472 [label=ToCopyBackward0]
	2617095736048 -> 2617095735472
	2616975764752 [label="layer1.1.prelu.weight
 (64)" fillcolor=lightblue]
	2616975764752 -> 2617095736048
	2617095736048 [label=AccumulateGrad]
	2617095735328 -> 2617095735184
	2617095735328 [label=ToCopyBackward0]
	2617095735904 -> 2617095735328
	2616975764992 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2616975764992 -> 2617095735904
	2617095735904 [label=AccumulateGrad]
	2617095735136 -> 2617095735040
	2616975764912 [label="layer1.1.bn3.weight
 (64)" fillcolor=lightblue]
	2616975764912 -> 2617095735136
	2617095735136 [label=AccumulateGrad]
	2617095735088 -> 2617095735040
	2616975765072 [label="layer1.1.bn3.bias
 (64)" fillcolor=lightblue]
	2616975765072 -> 2617095735088
	2617095735088 [label=AccumulateGrad]
	2617095734992 -> 2617095733888
	2617095734896 -> 2617095734752
	2616975765392 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2616975765392 -> 2617095734896
	2617095734896 [label=AccumulateGrad]
	2617095734848 -> 2617095734752
	2616975835200 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2616975835200 -> 2617095734848
	2617095734848 [label=AccumulateGrad]
	2617095734704 -> 2617095734560
	2617095734704 [label=ToCopyBackward0]
	2617095735280 -> 2617095734704
	2616975835680 [label="layer1.2.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2616975835680 -> 2617095735280
	2617095735280 [label=AccumulateGrad]
	2617095734512 -> 2617095734416
	2616975835600 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2616975835600 -> 2617095734512
	2617095734512 [label=AccumulateGrad]
	2617095734464 -> 2617095734416
	2616975835760 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2616975835760 -> 2617095734464
	2617095734464 [label=AccumulateGrad]
	2617095734368 -> 2617095734272
	2617095734368 [label=ToCopyBackward0]
	2617095734944 -> 2617095734368
	2616975836080 [label="layer1.2.prelu.weight
 (64)" fillcolor=lightblue]
	2616975836080 -> 2617095734944
	2617095734944 [label=AccumulateGrad]
	2617095734224 -> 2617095734080
	2617095734224 [label=ToCopyBackward0]
	2617095735760 -> 2617095734224
	2616975836320 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2616975836320 -> 2617095735760
	2617095735760 [label=AccumulateGrad]
	2617095734032 -> 2617095733936
	2616975836240 [label="layer1.2.bn3.weight
 (64)" fillcolor=lightblue]
	2616975836240 -> 2617095734032
	2617095734032 [label=AccumulateGrad]
	2617095733984 -> 2617095733936
	2616975836400 [label="layer1.2.bn3.bias
 (64)" fillcolor=lightblue]
	2616975836400 -> 2617095733984
	2617095733984 [label=AccumulateGrad]
	2617095733888 -> 2617095733744
	2617095733696 -> 2617095733600
	2616975837280 [label="layer2.0.bn1.weight
 (64)" fillcolor=lightblue]
	2616975837280 -> 2617095733696
	2617095733696 [label=AccumulateGrad]
	2617095733648 -> 2617095733600
	2616975837360 [label="layer2.0.bn1.bias
 (64)" fillcolor=lightblue]
	2616975837360 -> 2617095733648
	2617095733648 [label=AccumulateGrad]
	2617095733552 -> 2617095733408
	2617095733552 [label=ToCopyBackward0]
	2617095734128 -> 2617095733552
	2616975837840 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2616975837840 -> 2617095734128
	2617095734128 [label=AccumulateGrad]
	2617095733360 -> 2617095720912
	2616975837760 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2616975837760 -> 2617095733360
	2617095733360 [label=AccumulateGrad]
	2617095733312 -> 2617095720912
	2616975837920 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2616975837920 -> 2617095733312
	2617095733312 [label=AccumulateGrad]
	2617095720864 -> 2617095720768
	2617095720864 [label=ToCopyBackward0]
	2617095733840 -> 2617095720864
	2616975838240 [label="layer2.0.prelu.weight
 (128)" fillcolor=lightblue]
	2616975838240 -> 2617095733840
	2617095733840 [label=AccumulateGrad]
	2617095720720 -> 2617095720576
	2617095720720 [label=ToCopyBackward0]
	2617095720816 -> 2617095720720
	2616975838480 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2616975838480 -> 2617095720816
	2617095720816 [label=AccumulateGrad]
	2617095720528 -> 2617095720432
	2616975838400 [label="layer2.0.bn3.weight
 (128)" fillcolor=lightblue]
	2616975838400 -> 2617095720528
	2617095720528 [label=AccumulateGrad]
	2617095720480 -> 2617095720432
	2616975838560 [label="layer2.0.bn3.bias
 (128)" fillcolor=lightblue]
	2616975838560 -> 2617095720480
	2617095720480 [label=AccumulateGrad]
	2617095720384 -> 2617095719280
	2617095720384 [label=CudnnBatchNormBackward0]
	2617095720672 -> 2617095720384
	2617095720672 [label=ConvolutionBackward0]
	2617095733744 -> 2617095720672
	2617095733792 -> 2617095720672
	2617095733792 [label=ToCopyBackward0]
	2617095735712 -> 2617095733792
	2616975836800 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2616975836800 -> 2617095735712
	2617095735712 [label=AccumulateGrad]
	2617095720624 -> 2617095720384
	2616975836880 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2616975836880 -> 2617095720624
	2617095720624 [label=AccumulateGrad]
	2617095734176 -> 2617095720384
	2616975836960 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2616975836960 -> 2617095734176
	2617095734176 [label=AccumulateGrad]
	2617095720288 -> 2617095720144
	2616975838880 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2616975838880 -> 2617095720288
	2617095720288 [label=AccumulateGrad]
	2617095720240 -> 2617095720144
	2616975838960 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2616975838960 -> 2617095720240
	2617095720240 [label=AccumulateGrad]
	2617095720096 -> 2617095719952
	2617095720096 [label=ToCopyBackward0]
	2617095720336 -> 2617095720096
	2616975905072 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2616975905072 -> 2617095720336
	2617095720336 [label=AccumulateGrad]
	2617095719904 -> 2617095719808
	2616975904992 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2616975904992 -> 2617095719904
	2617095719904 [label=AccumulateGrad]
	2617095719856 -> 2617095719808
	2616975905152 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2616975905152 -> 2617095719856
	2617095719856 [label=AccumulateGrad]
	2617095719760 -> 2617095719664
	2617095719760 [label=ToCopyBackward0]
	2617095720192 -> 2617095719760
	2616975905472 [label="layer2.1.prelu.weight
 (128)" fillcolor=lightblue]
	2616975905472 -> 2617095720192
	2617095720192 [label=AccumulateGrad]
	2617095719616 -> 2617095719472
	2617095719616 [label=ToCopyBackward0]
	2617095720000 -> 2617095719616
	2616975905712 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2616975905712 -> 2617095720000
	2617095720000 [label=AccumulateGrad]
	2617095719424 -> 2617095719328
	2616975905632 [label="layer2.1.bn3.weight
 (128)" fillcolor=lightblue]
	2616975905632 -> 2617095719424
	2617095719424 [label=AccumulateGrad]
	2617095719376 -> 2617095719328
	2616975905792 [label="layer2.1.bn3.bias
 (128)" fillcolor=lightblue]
	2616975905792 -> 2617095719376
	2617095719376 [label=AccumulateGrad]
	2617095719280 -> 2617095718176
	2617095719184 -> 2617095719040
	2616975906112 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2616975906112 -> 2617095719184
	2617095719184 [label=AccumulateGrad]
	2617095719136 -> 2617095719040
	2616975906192 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2616975906192 -> 2617095719136
	2617095719136 [label=AccumulateGrad]
	2617095718992 -> 2617095718848
	2617095718992 [label=ToCopyBackward0]
	2617095719520 -> 2617095718992
	2616975906672 [label="layer2.2.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2616975906672 -> 2617095719520
	2617095719520 [label=AccumulateGrad]
	2617095718800 -> 2617095718704
	2616975906592 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2616975906592 -> 2617095718800
	2617095718800 [label=AccumulateGrad]
	2617095718752 -> 2617095718704
	2616975906752 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2616975906752 -> 2617095718752
	2617095718752 [label=AccumulateGrad]
	2617095718656 -> 2617095718560
	2617095718656 [label=ToCopyBackward0]
	2617095719232 -> 2617095718656
	2616975907072 [label="layer2.2.prelu.weight
 (128)" fillcolor=lightblue]
	2616975907072 -> 2617095719232
	2617095719232 [label=AccumulateGrad]
	2617095718512 -> 2617095718368
	2617095718512 [label=ToCopyBackward0]
	2617095719568 -> 2617095718512
	2616975907312 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2616975907312 -> 2617095719568
	2617095719568 [label=AccumulateGrad]
	2617095718320 -> 2617095718224
	2616975907232 [label="layer2.2.bn3.weight
 (128)" fillcolor=lightblue]
	2616975907232 -> 2617095718320
	2617095718320 [label=AccumulateGrad]
	2617095718272 -> 2617095718224
	2616975907392 [label="layer2.2.bn3.bias
 (128)" fillcolor=lightblue]
	2616975907392 -> 2617095718272
	2617095718272 [label=AccumulateGrad]
	2617095718176 -> 2617095717072
	2617095718080 -> 2617095717936
	2616975907712 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2616975907712 -> 2617095718080
	2617095718080 [label=AccumulateGrad]
	2617095718032 -> 2617095717936
	2616975907792 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2616975907792 -> 2617095718032
	2617095718032 [label=AccumulateGrad]
	2617095717888 -> 2617095717744
	2617095717888 [label=ToCopyBackward0]
	2617095718416 -> 2617095717888
	2616975908272 [label="layer2.3.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2616975908272 -> 2617095718416
	2617095718416 [label=AccumulateGrad]
	2617095717696 -> 2617095717600
	2616975908192 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2616975908192 -> 2617095717696
	2617095717696 [label=AccumulateGrad]
	2617095717648 -> 2617095717600
	2616975908352 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2616975908352 -> 2617095717648
	2617095717648 [label=AccumulateGrad]
	2617095717552 -> 2617095717456
	2617095717552 [label=ToCopyBackward0]
	2617095718128 -> 2617095717552
	2616975908672 [label="layer2.3.prelu.weight
 (128)" fillcolor=lightblue]
	2616975908672 -> 2617095718128
	2617095718128 [label=AccumulateGrad]
	2617095717408 -> 2617095717264
	2617095717408 [label=ToCopyBackward0]
	2617095718464 -> 2617095717408
	2616975974544 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2616975974544 -> 2617095718464
	2617095718464 [label=AccumulateGrad]
	2617095717216 -> 2617095717120
	2616975974464 [label="layer2.3.bn3.weight
 (128)" fillcolor=lightblue]
	2616975974464 -> 2617095717216
	2617095717216 [label=AccumulateGrad]
	2617095717168 -> 2617095717120
	2616975974624 [label="layer2.3.bn3.bias
 (128)" fillcolor=lightblue]
	2616975974624 -> 2617095717168
	2617095717168 [label=AccumulateGrad]
	2617095717072 -> 2617095704528
	2617095704480 -> 2617095704432
	2616975975504 [label="layer3.0.bn1.weight
 (128)" fillcolor=lightblue]
	2616975975504 -> 2617095704480
	2617095704480 [label=AccumulateGrad]
	2617095716928 -> 2617095704432
	2616975975584 [label="layer3.0.bn1.bias
 (128)" fillcolor=lightblue]
	2616975975584 -> 2617095716928
	2617095716928 [label=AccumulateGrad]
	2617095704384 -> 2617095704240
	2617095704384 [label=ToCopyBackward0]
	2617095717312 -> 2617095704384
	2616975976064 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2616975976064 -> 2617095717312
	2617095717312 [label=AccumulateGrad]
	2617095704192 -> 2617095704096
	2616975975984 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2616975975984 -> 2617095704192
	2617095704192 [label=AccumulateGrad]
	2617095704144 -> 2617095704096
	2616975976144 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2616975976144 -> 2617095704144
	2617095704144 [label=AccumulateGrad]
	2617095704048 -> 2617095703952
	2617095704048 [label=ToCopyBackward0]
	2617095704336 -> 2617095704048
	2616975976464 [label="layer3.0.prelu.weight
 (256)" fillcolor=lightblue]
	2616975976464 -> 2617095704336
	2617095704336 [label=AccumulateGrad]
	2617095703904 -> 2617095703760
	2617095703904 [label=ToCopyBackward0]
	2617095704288 -> 2617095703904
	2616975976704 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2616975976704 -> 2617095704288
	2617095704288 [label=AccumulateGrad]
	2617095703712 -> 2617095703616
	2616975976624 [label="layer3.0.bn3.weight
 (256)" fillcolor=lightblue]
	2616975976624 -> 2617095703712
	2617095703712 [label=AccumulateGrad]
	2617095703664 -> 2617095703616
	2616975976784 [label="layer3.0.bn3.bias
 (256)" fillcolor=lightblue]
	2616975976784 -> 2617095703664
	2617095703664 [label=AccumulateGrad]
	2617095703568 -> 2617095702464
	2617095703568 [label=CudnnBatchNormBackward0]
	2617095704000 -> 2617095703568
	2617095704000 [label=ConvolutionBackward0]
	2617095704528 -> 2617095704000
	2617095716976 -> 2617095704000
	2617095716976 [label=ToCopyBackward0]
	2617095718944 -> 2617095716976
	2616975975024 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2616975975024 -> 2617095718944
	2617095718944 [label=AccumulateGrad]
	2617095703856 -> 2617095703568
	2616975975104 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2616975975104 -> 2617095703856
	2617095703856 [label=AccumulateGrad]
	2617095703808 -> 2617095703568
	2616975975184 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2616975975184 -> 2617095703808
	2617095703808 [label=AccumulateGrad]
	2617095703472 -> 2617095703328
	2616975977104 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2616975977104 -> 2617095703472
	2617095703472 [label=AccumulateGrad]
	2617095703424 -> 2617095703328
	2616975977184 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2616975977184 -> 2617095703424
	2617095703424 [label=AccumulateGrad]
	2617095703280 -> 2617095703136
	2617095703280 [label=ToCopyBackward0]
	2617095703520 -> 2617095703280
	2616975977664 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2616975977664 -> 2617095703520
	2617095703520 [label=AccumulateGrad]
	2617095703088 -> 2617095702992
	2616975977584 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2616975977584 -> 2617095703088
	2617095703088 [label=AccumulateGrad]
	2617095703040 -> 2617095702992
	2616975977744 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2616975977744 -> 2617095703040
	2617095703040 [label=AccumulateGrad]
	2617095702944 -> 2617095702848
	2617095702944 [label=ToCopyBackward0]
	2617095703376 -> 2617095702944
	2616975978064 [label="layer3.1.prelu.weight
 (256)" fillcolor=lightblue]
	2616975978064 -> 2617095703376
	2617095703376 [label=AccumulateGrad]
	2617095702800 -> 2617095702656
	2617095702800 [label=ToCopyBackward0]
	2617095703184 -> 2617095702800
	2616975978304 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2616975978304 -> 2617095703184
	2617095703184 [label=AccumulateGrad]
	2617095702608 -> 2617095702512
	2616975978224 [label="layer3.1.bn3.weight
 (256)" fillcolor=lightblue]
	2616975978224 -> 2617095702608
	2617095702608 [label=AccumulateGrad]
	2617095702560 -> 2617095702512
	2616975978384 [label="layer3.1.bn3.bias
 (256)" fillcolor=lightblue]
	2616975978384 -> 2617095702560
	2617095702560 [label=AccumulateGrad]
	2617095702464 -> 2617095701360
	2617095702368 -> 2617095702224
	2616976048432 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2616976048432 -> 2617095702368
	2617095702368 [label=AccumulateGrad]
	2617095702320 -> 2617095702224
	2616976048512 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2616976048512 -> 2617095702320
	2617095702320 [label=AccumulateGrad]
	2617095702176 -> 2617095702032
	2617095702176 [label=ToCopyBackward0]
	2617095702704 -> 2617095702176
	2616976048992 [label="layer3.2.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2616976048992 -> 2617095702704
	2617095702704 [label=AccumulateGrad]
	2617095701984 -> 2617095701888
	2616976048912 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2616976048912 -> 2617095701984
	2617095701984 [label=AccumulateGrad]
	2617095701936 -> 2617095701888
	2616976049072 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2616976049072 -> 2617095701936
	2617095701936 [label=AccumulateGrad]
	2617095701840 -> 2617095701744
	2617095701840 [label=ToCopyBackward0]
	2617095702416 -> 2617095701840
	2616976049392 [label="layer3.2.prelu.weight
 (256)" fillcolor=lightblue]
	2616976049392 -> 2617095702416
	2617095702416 [label=AccumulateGrad]
	2617095701696 -> 2617095701552
	2617095701696 [label=ToCopyBackward0]
	2617095702752 -> 2617095701696
	2616976049632 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2616976049632 -> 2617095702752
	2617095702752 [label=AccumulateGrad]
	2617095701504 -> 2617095701408
	2616976049552 [label="layer3.2.bn3.weight
 (256)" fillcolor=lightblue]
	2616976049552 -> 2617095701504
	2617095701504 [label=AccumulateGrad]
	2617095701456 -> 2617095701408
	2616976049712 [label="layer3.2.bn3.bias
 (256)" fillcolor=lightblue]
	2616976049712 -> 2617095701456
	2617095701456 [label=AccumulateGrad]
	2617095701360 -> 2617095687904
	2617095701264 -> 2617095701120
	2616976050032 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2616976050032 -> 2617095701264
	2617095701264 [label=AccumulateGrad]
	2617095701216 -> 2617095701120
	2616976050112 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2616976050112 -> 2617095701216
	2617095701216 [label=AccumulateGrad]
	2617095701072 -> 2617095700928
	2617095701072 [label=ToCopyBackward0]
	2617095701600 -> 2617095701072
	2616976050592 [label="layer3.3.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2616976050592 -> 2617095701600
	2617095701600 [label=AccumulateGrad]
	2617095700880 -> 2617095700784
	2616976050512 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2616976050512 -> 2617095700880
	2617095700880 [label=AccumulateGrad]
	2617095700832 -> 2617095700784
	2616976050672 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2616976050672 -> 2617095700832
	2617095700832 [label=AccumulateGrad]
	2617095700736 -> 2617095700640
	2617095700736 [label=ToCopyBackward0]
	2617095701312 -> 2617095700736
	2616976050992 [label="layer3.3.prelu.weight
 (256)" fillcolor=lightblue]
	2616976050992 -> 2617095701312
	2617095701312 [label=AccumulateGrad]
	2617095700592 -> 2617095688096
	2617095700592 [label=ToCopyBackward0]
	2617095701648 -> 2617095700592
	2616976051232 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2616976051232 -> 2617095701648
	2617095701648 [label=AccumulateGrad]
	2617095688048 -> 2617095687952
	2616976051152 [label="layer3.3.bn3.weight
 (256)" fillcolor=lightblue]
	2616976051152 -> 2617095688048
	2617095688048 [label=AccumulateGrad]
	2617095688000 -> 2617095687952
	2616976051312 [label="layer3.3.bn3.bias
 (256)" fillcolor=lightblue]
	2616976051312 -> 2617095688000
	2617095688000 [label=AccumulateGrad]
	2617095687904 -> 2617095686800
	2617095687808 -> 2617095687664
	2616976051632 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2616976051632 -> 2617095687808
	2617095687808 [label=AccumulateGrad]
	2617095687760 -> 2617095687664
	2616976051712 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2616976051712 -> 2617095687760
	2617095687760 [label=AccumulateGrad]
	2617095687616 -> 2617095687472
	2617095687616 [label=ToCopyBackward0]
	2617095688144 -> 2617095687616
	2616976113728 [label="layer3.4.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2616976113728 -> 2617095688144
	2617095688144 [label=AccumulateGrad]
	2617095687424 -> 2617095687328
	2616976052112 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2616976052112 -> 2617095687424
	2617095687424 [label=AccumulateGrad]
	2617095687376 -> 2617095687328
	2616976113808 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2616976113808 -> 2617095687376
	2617095687376 [label=AccumulateGrad]
	2617095687280 -> 2617095687184
	2617095687280 [label=ToCopyBackward0]
	2617095687856 -> 2617095687280
	2616976114128 [label="layer3.4.prelu.weight
 (256)" fillcolor=lightblue]
	2616976114128 -> 2617095687856
	2617095687856 [label=AccumulateGrad]
	2617095687136 -> 2617095686992
	2617095687136 [label=ToCopyBackward0]
	2617095687712 -> 2617095687136
	2616976114368 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2616976114368 -> 2617095687712
	2617095687712 [label=AccumulateGrad]
	2617095686944 -> 2617095686848
	2616976114288 [label="layer3.4.bn3.weight
 (256)" fillcolor=lightblue]
	2616976114288 -> 2617095686944
	2617095686944 [label=AccumulateGrad]
	2617095686896 -> 2617095686848
	2616976114448 [label="layer3.4.bn3.bias
 (256)" fillcolor=lightblue]
	2616976114448 -> 2617095686896
	2617095686896 [label=AccumulateGrad]
	2617095686800 -> 2617095685696
	2617095686704 -> 2617095686560
	2616976114768 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2616976114768 -> 2617095686704
	2617095686704 [label=AccumulateGrad]
	2617095686656 -> 2617095686560
	2616976114848 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2616976114848 -> 2617095686656
	2617095686656 [label=AccumulateGrad]
	2617095686512 -> 2617095686368
	2617095686512 [label=ToCopyBackward0]
	2617095687040 -> 2617095686512
	2616976115328 [label="layer3.5.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2616976115328 -> 2617095687040
	2617095687040 [label=AccumulateGrad]
	2617095686320 -> 2617095686224
	2616976115248 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2616976115248 -> 2617095686320
	2617095686320 [label=AccumulateGrad]
	2617095686272 -> 2617095686224
	2616976115408 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2616976115408 -> 2617095686272
	2617095686272 [label=AccumulateGrad]
	2617095686176 -> 2617095686080
	2617095686176 [label=ToCopyBackward0]
	2617095686752 -> 2617095686176
	2616976115728 [label="layer3.5.prelu.weight
 (256)" fillcolor=lightblue]
	2616976115728 -> 2617095686752
	2617095686752 [label=AccumulateGrad]
	2617095686032 -> 2617095685888
	2617095686032 [label=ToCopyBackward0]
	2617095687088 -> 2617095686032
	2616976115968 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2616976115968 -> 2617095687088
	2617095687088 [label=AccumulateGrad]
	2617095685840 -> 2617095685744
	2616976115888 [label="layer3.5.bn3.weight
 (256)" fillcolor=lightblue]
	2616976115888 -> 2617095685840
	2617095685840 [label=AccumulateGrad]
	2617095685792 -> 2617095685744
	2616976116048 [label="layer3.5.bn3.bias
 (256)" fillcolor=lightblue]
	2616976116048 -> 2617095685792
	2617095685792 [label=AccumulateGrad]
	2617095685696 -> 2617095684592
	2617095685600 -> 2617095685456
	2616976116368 [label="layer3.6.bn1.weight
 (256)" fillcolor=lightblue]
	2616976116368 -> 2617095685600
	2617095685600 [label=AccumulateGrad]
	2617095685552 -> 2617095685456
	2616976116448 [label="layer3.6.bn1.bias
 (256)" fillcolor=lightblue]
	2616976116448 -> 2617095685552
	2617095685552 [label=AccumulateGrad]
	2617095685408 -> 2617095685264
	2617095685408 [label=ToCopyBackward0]
	2617095685936 -> 2617095685408
	2616976116928 [label="layer3.6.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2616976116928 -> 2617095685936
	2617095685936 [label=AccumulateGrad]
	2617095685216 -> 2617095685120
	2616976116848 [label="layer3.6.bn2.weight
 (256)" fillcolor=lightblue]
	2616976116848 -> 2617095685216
	2617095685216 [label=AccumulateGrad]
	2617095685168 -> 2617095685120
	2616976117008 [label="layer3.6.bn2.bias
 (256)" fillcolor=lightblue]
	2616976117008 -> 2617095685168
	2617095685168 [label=AccumulateGrad]
	2617095685072 -> 2617095684976
	2617095685072 [label=ToCopyBackward0]
	2617095685360 -> 2617095685072
	2616976117328 [label="layer3.6.prelu.weight
 (256)" fillcolor=lightblue]
	2616976117328 -> 2617095685360
	2617095685360 [label=AccumulateGrad]
	2617095684928 -> 2617095684784
	2617095684928 [label=ToCopyBackward0]
	2617095685504 -> 2617095684928
	2616976117568 [label="layer3.6.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2616976117568 -> 2617095685504
	2617095685504 [label=AccumulateGrad]
	2617095684736 -> 2617095684640
	2616976117488 [label="layer3.6.bn3.weight
 (256)" fillcolor=lightblue]
	2616976117488 -> 2617095684736
	2617095684736 [label=AccumulateGrad]
	2617095684688 -> 2617095684640
	2616976117648 [label="layer3.6.bn3.bias
 (256)" fillcolor=lightblue]
	2616976117648 -> 2617095684688
	2617095684688 [label=AccumulateGrad]
	2617095684592 -> 2617095675232
	2617095684496 -> 2617095684352
	2616976191792 [label="layer3.7.bn1.weight
 (256)" fillcolor=lightblue]
	2616976191792 -> 2617095684496
	2617095684496 [label=AccumulateGrad]
	2617095684448 -> 2617095684352
	2616976191872 [label="layer3.7.bn1.bias
 (256)" fillcolor=lightblue]
	2616976191872 -> 2617095684448
	2617095684448 [label=AccumulateGrad]
	2617095684304 -> 2617095675856
	2617095684304 [label=ToCopyBackward0]
	2617095684832 -> 2617095684304
	2616976192352 [label="layer3.7.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2616976192352 -> 2617095684832
	2617095684832 [label=AccumulateGrad]
	2617095675808 -> 2617095675760
	2616976192272 [label="layer3.7.bn2.weight
 (256)" fillcolor=lightblue]
	2616976192272 -> 2617095675808
	2617095675808 [label=AccumulateGrad]
	2617095684160 -> 2617095675760
	2616976192432 [label="layer3.7.bn2.bias
 (256)" fillcolor=lightblue]
	2616976192432 -> 2617095684160
	2617095684160 [label=AccumulateGrad]
	2617095675712 -> 2617095675616
	2617095675712 [label=ToCopyBackward0]
	2617095684544 -> 2617095675712
	2616976192752 [label="layer3.7.prelu.weight
 (256)" fillcolor=lightblue]
	2616976192752 -> 2617095684544
	2617095684544 [label=AccumulateGrad]
	2617095675568 -> 2617095675424
	2617095675568 [label=ToCopyBackward0]
	2617095675664 -> 2617095675568
	2616976192992 [label="layer3.7.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2616976192992 -> 2617095675664
	2617095675664 [label=AccumulateGrad]
	2617095675376 -> 2617095675280
	2616976192912 [label="layer3.7.bn3.weight
 (256)" fillcolor=lightblue]
	2616976192912 -> 2617095675376
	2617095675376 [label=AccumulateGrad]
	2617095675328 -> 2617095675280
	2616976193072 [label="layer3.7.bn3.bias
 (256)" fillcolor=lightblue]
	2616976193072 -> 2617095675328
	2617095675328 [label=AccumulateGrad]
	2617095675232 -> 2617095674128
	2617095675136 -> 2617095674992
	2616976193392 [label="layer3.8.bn1.weight
 (256)" fillcolor=lightblue]
	2616976193392 -> 2617095675136
	2617095675136 [label=AccumulateGrad]
	2617095675088 -> 2617095674992
	2616976193472 [label="layer3.8.bn1.bias
 (256)" fillcolor=lightblue]
	2616976193472 -> 2617095675088
	2617095675088 [label=AccumulateGrad]
	2617095674944 -> 2617095674800
	2617095674944 [label=ToCopyBackward0]
	2617095675472 -> 2617095674944
	2616976193952 [label="layer3.8.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2616976193952 -> 2617095675472
	2617095675472 [label=AccumulateGrad]
	2617095674752 -> 2617095674656
	2616976193872 [label="layer3.8.bn2.weight
 (256)" fillcolor=lightblue]
	2616976193872 -> 2617095674752
	2617095674752 [label=AccumulateGrad]
	2617095674704 -> 2617095674656
	2616976194032 [label="layer3.8.bn2.bias
 (256)" fillcolor=lightblue]
	2616976194032 -> 2617095674704
	2617095674704 [label=AccumulateGrad]
	2617095674608 -> 2617095674512
	2617095674608 [label=ToCopyBackward0]
	2617095675184 -> 2617095674608
	2616976194352 [label="layer3.8.prelu.weight
 (256)" fillcolor=lightblue]
	2616976194352 -> 2617095675184
	2617095675184 [label=AccumulateGrad]
	2617095674464 -> 2617095674320
	2617095674464 [label=ToCopyBackward0]
	2617095675040 -> 2617095674464
	2616976194592 [label="layer3.8.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2616976194592 -> 2617095675040
	2617095675040 [label=AccumulateGrad]
	2617095674272 -> 2617095674176
	2616976194512 [label="layer3.8.bn3.weight
 (256)" fillcolor=lightblue]
	2616976194512 -> 2617095674272
	2617095674272 [label=AccumulateGrad]
	2617095674224 -> 2617095674176
	2616976194672 [label="layer3.8.bn3.bias
 (256)" fillcolor=lightblue]
	2616976194672 -> 2617095674224
	2617095674224 [label=AccumulateGrad]
	2617095674128 -> 2617095673024
	2617095674032 -> 2617095673888
	2616976194992 [label="layer3.9.bn1.weight
 (256)" fillcolor=lightblue]
	2616976194992 -> 2617095674032
	2617095674032 [label=AccumulateGrad]
	2617095673984 -> 2617095673888
	2616976195072 [label="layer3.9.bn1.bias
 (256)" fillcolor=lightblue]
	2616976195072 -> 2617095673984
	2617095673984 [label=AccumulateGrad]
	2617095673840 -> 2617095673696
	2617095673840 [label=ToCopyBackward0]
	2617095674368 -> 2617095673840
	2617013870656 [label="layer3.9.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2617013870656 -> 2617095674368
	2617095674368 [label=AccumulateGrad]
	2617095673648 -> 2617095673552
	2616976195472 [label="layer3.9.bn2.weight
 (256)" fillcolor=lightblue]
	2616976195472 -> 2617095673648
	2617095673648 [label=AccumulateGrad]
	2617095673600 -> 2617095673552
	2617013870736 [label="layer3.9.bn2.bias
 (256)" fillcolor=lightblue]
	2617013870736 -> 2617095673600
	2617095673600 [label=AccumulateGrad]
	2617095673504 -> 2617095673408
	2617095673504 [label=ToCopyBackward0]
	2617095674080 -> 2617095673504
	2617013871056 [label="layer3.9.prelu.weight
 (256)" fillcolor=lightblue]
	2617013871056 -> 2617095674080
	2617095674080 [label=AccumulateGrad]
	2617095673360 -> 2617095673216
	2617095673360 [label=ToCopyBackward0]
	2617095674416 -> 2617095673360
	2617013871296 [label="layer3.9.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2617013871296 -> 2617095674416
	2617095674416 [label=AccumulateGrad]
	2617095673168 -> 2617095673072
	2617013871216 [label="layer3.9.bn3.weight
 (256)" fillcolor=lightblue]
	2617013871216 -> 2617095673168
	2617095673168 [label=AccumulateGrad]
	2617095673120 -> 2617095673072
	2617013871376 [label="layer3.9.bn3.bias
 (256)" fillcolor=lightblue]
	2617013871376 -> 2617095673120
	2617095673120 [label=AccumulateGrad]
	2617095673024 -> 2617095671920
	2617095672928 -> 2617095672784
	2617013871696 [label="layer3.10.bn1.weight
 (256)" fillcolor=lightblue]
	2617013871696 -> 2617095672928
	2617095672928 [label=AccumulateGrad]
	2617095672880 -> 2617095672784
	2617013871776 [label="layer3.10.bn1.bias
 (256)" fillcolor=lightblue]
	2617013871776 -> 2617095672880
	2617095672880 [label=AccumulateGrad]
	2617095672736 -> 2617095672592
	2617095672736 [label=ToCopyBackward0]
	2617095673264 -> 2617095672736
	2617013872256 [label="layer3.10.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2617013872256 -> 2617095673264
	2617095673264 [label=AccumulateGrad]
	2617095672544 -> 2617095672448
	2617013872176 [label="layer3.10.bn2.weight
 (256)" fillcolor=lightblue]
	2617013872176 -> 2617095672544
	2617095672544 [label=AccumulateGrad]
	2617095672496 -> 2617095672448
	2617013872336 [label="layer3.10.bn2.bias
 (256)" fillcolor=lightblue]
	2617013872336 -> 2617095672496
	2617095672496 [label=AccumulateGrad]
	2617095672400 -> 2617095672304
	2617095672400 [label=ToCopyBackward0]
	2617095672976 -> 2617095672400
	2617013872656 [label="layer3.10.prelu.weight
 (256)" fillcolor=lightblue]
	2617013872656 -> 2617095672976
	2617095672976 [label=AccumulateGrad]
	2617095672256 -> 2617095672112
	2617095672256 [label=ToCopyBackward0]
	2617095673312 -> 2617095672256
	2617013872896 [label="layer3.10.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2617013872896 -> 2617095673312
	2617095673312 [label=AccumulateGrad]
	2617095672064 -> 2617095671968
	2617013872816 [label="layer3.10.bn3.weight
 (256)" fillcolor=lightblue]
	2617013872816 -> 2617095672064
	2617095672064 [label=AccumulateGrad]
	2617095672016 -> 2617095671968
	2617013872976 [label="layer3.10.bn3.bias
 (256)" fillcolor=lightblue]
	2617013872976 -> 2617095672016
	2617095672016 [label=AccumulateGrad]
	2617095671920 -> 2617095658464
	2617095659472 -> 2617095659328
	2617013873296 [label="layer3.11.bn1.weight
 (256)" fillcolor=lightblue]
	2617013873296 -> 2617095659472
	2617095659472 [label=AccumulateGrad]
	2617095659424 -> 2617095659328
	2617013873376 [label="layer3.11.bn1.bias
 (256)" fillcolor=lightblue]
	2617013873376 -> 2617095659424
	2617095659424 [label=AccumulateGrad]
	2617095659280 -> 2617095659136
	2617095659280 [label=ToCopyBackward0]
	2617095659376 -> 2617095659280
	2617013873856 [label="layer3.11.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2617013873856 -> 2617095659376
	2617095659376 [label=AccumulateGrad]
	2617095659088 -> 2617095658992
	2617013873776 [label="layer3.11.bn2.weight
 (256)" fillcolor=lightblue]
	2617013873776 -> 2617095659088
	2617095659088 [label=AccumulateGrad]
	2617095659040 -> 2617095658992
	2617013873936 [label="layer3.11.bn2.bias
 (256)" fillcolor=lightblue]
	2617013873936 -> 2617095659040
	2617095659040 [label=AccumulateGrad]
	2617095658944 -> 2617095658848
	2617095658944 [label=ToCopyBackward0]
	2617095659232 -> 2617095658944
	2617013874256 [label="layer3.11.prelu.weight
 (256)" fillcolor=lightblue]
	2617013874256 -> 2617095659232
	2617095659232 [label=AccumulateGrad]
	2617095658800 -> 2617095658656
	2617095658800 [label=ToCopyBackward0]
	2617095659184 -> 2617095658800
	2617013874496 [label="layer3.11.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2617013874496 -> 2617095659184
	2617095659184 [label=AccumulateGrad]
	2617095658608 -> 2617095658512
	2617013874416 [label="layer3.11.bn3.weight
 (256)" fillcolor=lightblue]
	2617013874416 -> 2617095658608
	2617095658608 [label=AccumulateGrad]
	2617095658560 -> 2617095658512
	2617013874576 [label="layer3.11.bn3.bias
 (256)" fillcolor=lightblue]
	2617013874576 -> 2617095658560
	2617095658560 [label=AccumulateGrad]
	2617095658464 -> 2617095657360
	2617095658368 -> 2617095658224
	2617013940528 [label="layer3.12.bn1.weight
 (256)" fillcolor=lightblue]
	2617013940528 -> 2617095658368
	2617095658368 [label=AccumulateGrad]
	2617095658320 -> 2617095658224
	2617013940608 [label="layer3.12.bn1.bias
 (256)" fillcolor=lightblue]
	2617013940608 -> 2617095658320
	2617095658320 [label=AccumulateGrad]
	2617095658176 -> 2617095658032
	2617095658176 [label=ToCopyBackward0]
	2617095658704 -> 2617095658176
	2617013941088 [label="layer3.12.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2617013941088 -> 2617095658704
	2617095658704 [label=AccumulateGrad]
	2617095657984 -> 2617095657888
	2617013941008 [label="layer3.12.bn2.weight
 (256)" fillcolor=lightblue]
	2617013941008 -> 2617095657984
	2617095657984 [label=AccumulateGrad]
	2617095657936 -> 2617095657888
	2617013941168 [label="layer3.12.bn2.bias
 (256)" fillcolor=lightblue]
	2617013941168 -> 2617095657936
	2617095657936 [label=AccumulateGrad]
	2617095657840 -> 2617095657744
	2617095657840 [label=ToCopyBackward0]
	2617095658416 -> 2617095657840
	2617013941488 [label="layer3.12.prelu.weight
 (256)" fillcolor=lightblue]
	2617013941488 -> 2617095658416
	2617095658416 [label=AccumulateGrad]
	2617095657696 -> 2617095657552
	2617095657696 [label=ToCopyBackward0]
	2617095658752 -> 2617095657696
	2617013941728 [label="layer3.12.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2617013941728 -> 2617095658752
	2617095658752 [label=AccumulateGrad]
	2617095657504 -> 2617095657408
	2617013941648 [label="layer3.12.bn3.weight
 (256)" fillcolor=lightblue]
	2617013941648 -> 2617095657504
	2617095657504 [label=AccumulateGrad]
	2617095657456 -> 2617095657408
	2617013941808 [label="layer3.12.bn3.bias
 (256)" fillcolor=lightblue]
	2617013941808 -> 2617095657456
	2617095657456 [label=AccumulateGrad]
	2617095657360 -> 2617095656256
	2617095657264 -> 2617095657120
	2617013942128 [label="layer3.13.bn1.weight
 (256)" fillcolor=lightblue]
	2617013942128 -> 2617095657264
	2617095657264 [label=AccumulateGrad]
	2617095657216 -> 2617095657120
	2617013942208 [label="layer3.13.bn1.bias
 (256)" fillcolor=lightblue]
	2617013942208 -> 2617095657216
	2617095657216 [label=AccumulateGrad]
	2617095657072 -> 2617095656928
	2617095657072 [label=ToCopyBackward0]
	2617095657600 -> 2617095657072
	2617013942688 [label="layer3.13.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2617013942688 -> 2617095657600
	2617095657600 [label=AccumulateGrad]
	2617095656880 -> 2617095656784
	2617013942608 [label="layer3.13.bn2.weight
 (256)" fillcolor=lightblue]
	2617013942608 -> 2617095656880
	2617095656880 [label=AccumulateGrad]
	2617095656832 -> 2617095656784
	2617013942768 [label="layer3.13.bn2.bias
 (256)" fillcolor=lightblue]
	2617013942768 -> 2617095656832
	2617095656832 [label=AccumulateGrad]
	2617095656736 -> 2617095656640
	2617095656736 [label=ToCopyBackward0]
	2617095657312 -> 2617095656736
	2617013943088 [label="layer3.13.prelu.weight
 (256)" fillcolor=lightblue]
	2617013943088 -> 2617095657312
	2617095657312 [label=AccumulateGrad]
	2617095656592 -> 2617095656448
	2617095656592 [label=ToCopyBackward0]
	2617095657648 -> 2617095656592
	2617013943328 [label="layer3.13.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2617013943328 -> 2617095657648
	2617095657648 [label=AccumulateGrad]
	2617095656400 -> 2617095656304
	2617013943248 [label="layer3.13.bn3.weight
 (256)" fillcolor=lightblue]
	2617013943248 -> 2617095656400
	2617095656400 [label=AccumulateGrad]
	2617095656352 -> 2617095656304
	2617013943408 [label="layer3.13.bn3.bias
 (256)" fillcolor=lightblue]
	2617013943408 -> 2617095656352
	2617095656352 [label=AccumulateGrad]
	2617095656256 -> 2617095656112
	2617095656064 -> 2617095655968
	2617014014016 [label="layer4.0.bn1.weight
 (256)" fillcolor=lightblue]
	2617014014016 -> 2617095656064
	2617095656064 [label=AccumulateGrad]
	2617095656016 -> 2617095655968
	2617014014096 [label="layer4.0.bn1.bias
 (256)" fillcolor=lightblue]
	2617014014096 -> 2617095656016
	2617095656016 [label=AccumulateGrad]
	2617095655920 -> 2617095655776
	2617095655920 [label=ToCopyBackward0]
	2617095656496 -> 2617095655920
	2617014014576 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2617014014576 -> 2617095656496
	2617095656496 [label=AccumulateGrad]
	2617095655728 -> 2617095655632
	2617014014496 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2617014014496 -> 2617095655728
	2617095655728 [label=AccumulateGrad]
	2617095655680 -> 2617095655632
	2617014014656 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2617014014656 -> 2617095655680
	2617095655680 [label=AccumulateGrad]
	2617095655584 -> 2617095589840
	2617095655584 [label=ToCopyBackward0]
	2617095656208 -> 2617095655584
	2617014014976 [label="layer4.0.prelu.weight
 (512)" fillcolor=lightblue]
	2617014014976 -> 2617095656208
	2617095656208 [label=AccumulateGrad]
	2617095589792 -> 2617095589696
	2617095589792 [label=ToCopyBackward0]
	2617095656544 -> 2617095589792
	2617014015216 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2617014015216 -> 2617095656544
	2617095656544 [label=AccumulateGrad]
	2617095589648 -> 2617095589552
	2617014015136 [label="layer4.0.bn3.weight
 (512)" fillcolor=lightblue]
	2617014015136 -> 2617095589648
	2617095589648 [label=AccumulateGrad]
	2617095589600 -> 2617095589552
	2617014015296 [label="layer4.0.bn3.bias
 (512)" fillcolor=lightblue]
	2617014015296 -> 2617095589600
	2617095589600 [label=AccumulateGrad]
	2617095589504 -> 2617095588400
	2617095589504 [label=CudnnBatchNormBackward0]
	2617095589744 -> 2617095589504
	2617095589744 [label=ConvolutionBackward0]
	2617095656112 -> 2617095589744
	2617095656160 -> 2617095589744
	2617095656160 [label=ToCopyBackward0]
	2617095658128 -> 2617095656160
	2617013943808 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2617013943808 -> 2617095658128
	2617095658128 [label=AccumulateGrad]
	2617095655872 -> 2617095589504
	2617013943888 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2617013943888 -> 2617095655872
	2617095655872 [label=AccumulateGrad]
	2617095655488 -> 2617095589504
	2617013943968 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2617013943968 -> 2617095655488
	2617095655488 [label=AccumulateGrad]
	2617095589408 -> 2617095589264
	2617014015616 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2617014015616 -> 2617095589408
	2617095589408 [label=AccumulateGrad]
	2617095589360 -> 2617095589264
	2617014015696 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2617014015696 -> 2617095589360
	2617095589360 [label=AccumulateGrad]
	2617095589216 -> 2617095589072
	2617095589216 [label=ToCopyBackward0]
	2617095589456 -> 2617095589216
	2617014016176 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2617014016176 -> 2617095589456
	2617095589456 [label=AccumulateGrad]
	2617095589024 -> 2617095588928
	2617014016096 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2617014016096 -> 2617095589024
	2617095589024 [label=AccumulateGrad]
	2617095588976 -> 2617095588928
	2617014016256 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2617014016256 -> 2617095588976
	2617095588976 [label=AccumulateGrad]
	2617095588880 -> 2617095588784
	2617095588880 [label=ToCopyBackward0]
	2617095589312 -> 2617095588880
	2617014016576 [label="layer4.1.prelu.weight
 (512)" fillcolor=lightblue]
	2617014016576 -> 2617095589312
	2617095589312 [label=AccumulateGrad]
	2617095588736 -> 2617095588592
	2617095588736 [label=ToCopyBackward0]
	2617095589120 -> 2617095588736
	2617014016816 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2617014016816 -> 2617095589120
	2617095589120 [label=AccumulateGrad]
	2617095588544 -> 2617095588448
	2617014016736 [label="layer4.1.bn3.weight
 (512)" fillcolor=lightblue]
	2617014016736 -> 2617095588544
	2617095588544 [label=AccumulateGrad]
	2617095588496 -> 2617095588448
	2617014016896 [label="layer4.1.bn3.bias
 (512)" fillcolor=lightblue]
	2617014016896 -> 2617095588496
	2617095588496 [label=AccumulateGrad]
	2617095588400 -> 2617095587296
	2617095588304 -> 2617095588160
	2617014017216 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2617014017216 -> 2617095588304
	2617095588304 [label=AccumulateGrad]
	2617095588256 -> 2617095588160
	2617014017296 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2617014017296 -> 2617095588256
	2617095588256 [label=AccumulateGrad]
	2617095588112 -> 2617095587968
	2617095588112 [label=ToCopyBackward0]
	2617095588640 -> 2617095588112
	2617014017776 [label="layer4.2.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2617014017776 -> 2617095588640
	2617095588640 [label=AccumulateGrad]
	2617095587920 -> 2617095587824
	2617014017696 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2617014017696 -> 2617095587920
	2617095587920 [label=AccumulateGrad]
	2617095587872 -> 2617095587824
	2617014017856 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2617014017856 -> 2617095587872
	2617095587872 [label=AccumulateGrad]
	2617095587776 -> 2617095587680
	2617095587776 [label=ToCopyBackward0]
	2617095588352 -> 2617095587776
	2617014079712 [label="layer4.2.prelu.weight
 (512)" fillcolor=lightblue]
	2617014079712 -> 2617095588352
	2617095588352 [label=AccumulateGrad]
	2617095587632 -> 2617095587488
	2617095587632 [label=ToCopyBackward0]
	2617095588688 -> 2617095587632
	2617014079952 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2617014079952 -> 2617095588688
	2617095588688 [label=AccumulateGrad]
	2617095587440 -> 2617095587344
	2617014080032 [label="layer4.2.bn3.weight
 (512)" fillcolor=lightblue]
	2617014080032 -> 2617095587440
	2617095587440 [label=AccumulateGrad]
	2617095587392 -> 2617095587344
	2617014080112 [label="layer4.2.bn3.bias
 (512)" fillcolor=lightblue]
	2617014080112 -> 2617095587392
	2617095587392 [label=AccumulateGrad]
	2617095587296 -> 2617095587152
	2617095587104 -> 2617095587056
	2617014080432 [label="bn2.weight
 (512)" fillcolor=lightblue]
	2617014080432 -> 2617095587104
	2617095587104 [label=AccumulateGrad]
	2617095586960 -> 2617095587056
	2617014080512 [label="bn2.bias
 (512)" fillcolor=lightblue]
	2617014080512 -> 2617095586960
	2617095586960 [label=AccumulateGrad]
	2617095586384 -> 2617095586480
	2617095586384 [label=TBackward0]
	2617095587200 -> 2617095586384
	2617014080752 [label="fc.weight
 (512, 25088)" fillcolor=lightblue]
	2617014080752 -> 2617095587200
	2617095587200 [label=AccumulateGrad]
	2617095586336 -> 2617095586672
	2617014080992 [label="features.bias
 (512)" fillcolor=lightblue]
	2617014080992 -> 2617095586336
	2617095586336 [label=AccumulateGrad]
	2617095586576 -> 2617095600304
}
